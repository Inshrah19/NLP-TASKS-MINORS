{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d129b9-af88-4f5e-a44f-b87326fb9fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install NLTK if needed (uncomment next line in Colab or Jupyter if required)\n",
    "# !pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')           # For tokenization\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('maxent_ne_chunker')           # For NER chunking\n",
    "nltk.download('words')           # For NER support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37bbf00-2eaf-4299-a039-4a93bc6faaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags for Sentence 1:\n",
      "Barack: NNP\n",
      "Obama: NNP\n",
      "visited: VBD\n",
      "Pakistan: NNP\n",
      "in: IN\n",
      "2010: CD\n",
      ".: .\n",
      "\n",
      "Named Entities for Sentence 1:\n",
      "Entity: Barack | Type: PERSON\n",
      "Entity: Obama | Type: PERSON\n",
      "Entity: Pakistan | Type: GPE\n",
      "----------------------------------------\n",
      "POS tags for Sentence 2:\n",
      "Apple: NNP\n",
      "is: VBZ\n",
      "looking: VBG\n",
      "at: IN\n",
      "buying: VBG\n",
      "a: DT\n",
      "UK: NNP\n",
      "startup: NN\n",
      "for: IN\n",
      "$: $\n",
      "1: CD\n",
      "billion: CD\n",
      ".: .\n",
      "\n",
      "Named Entities for Sentence 2:\n",
      "Entity: Apple | Type: GPE\n",
      "----------------------------------------\n",
      "POS tags for Sentence 3:\n",
      "The: DT\n",
      "quick: JJ\n",
      "brown: NN\n",
      "fox: NN\n",
      "jumps: VBZ\n",
      "over: IN\n",
      "the: DT\n",
      "lazy: JJ\n",
      "dog: NN\n",
      ".: .\n",
      "\n",
      "Named Entities for Sentence 3:\n",
      "No named entity found.\n",
      "----------------------------------------\n",
      "POS tags for Sentence 4:\n",
      "Tesla: NNP\n",
      "released: VBD\n",
      "its: PRP$\n",
      "new: JJ\n",
      "model: NN\n",
      "in: IN\n",
      "California: NNP\n",
      ".: .\n",
      "\n",
      "Named Entities for Sentence 4:\n",
      "Entity: Tesla | Type: PERSON\n",
      "Entity: California | Type: GPE\n",
      "----------------------------------------\n",
      "POS tags for Sentence 5:\n",
      "Lionel: NNP\n",
      "Messi: NNP\n",
      "plays: VBZ\n",
      "for: IN\n",
      "Inter: NNP\n",
      "Miami: NNP\n",
      ".: .\n",
      "\n",
      "Named Entities for Sentence 5:\n",
      "Entity: Lionel | Type: PERSON\n",
      "Entity: Messi | Type: ORGANIZATION\n",
      "Entity: Inter Miami | Type: ORGANIZATION\n",
      "----------------------------------------\n",
      "POS tags for Sentence 6:\n",
      "The: DT\n",
      "United: NNP\n",
      "Nations: NNPS\n",
      "held: VBD\n",
      "a: DT\n",
      "meeting: NN\n",
      "in: IN\n",
      "New: NNP\n",
      "York: NNP\n",
      ".: .\n",
      "\n",
      "Named Entities for Sentence 6:\n",
      "Entity: United Nations | Type: ORGANIZATION\n",
      "Entity: New York | Type: GPE\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sentences = [\n",
    "    \"Barack Obama visited Pakistan in 2010.\",\n",
    "    \"Apple is looking at buying a UK startup for $1 billion.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Tesla released its new model in California.\",\n",
    "    \"Lionel Messi plays for Inter Miami.\",\n",
    "    \"The United Nations held a meeting in New York.\",\n",
    "]\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "for idx, sent in enumerate(sentences, 1):\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    \n",
    "    # --- POS TAGGING OUTPUT ---\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    print(f\"POS tags for Sentence {idx}:\")\n",
    "    for token, tag in pos_tags:\n",
    "        print(f\"{token}: {tag}\")\n",
    "    \n",
    "    # --- NER OUTPUT ---\n",
    "    ner_tree = nltk.ne_chunk(pos_tags)\n",
    "    print(f\"\\nNamed Entities for Sentence {idx}:\")\n",
    "    found_entity = False\n",
    "    for subtree in ner_tree:\n",
    "        if hasattr(subtree, 'label'):\n",
    "            entity = ' '.join(token for token, pos in subtree.leaves())\n",
    "            label = subtree.label()\n",
    "            print(f\"Entity: {entity} | Type: {label}\")\n",
    "            found_entity = True\n",
    "    if not found_entity:\n",
    "        print(\"No named entity found.\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1044f8bb-d274-4897-a6c8-29842ab0a953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\apps\\anoconda\\lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\apps\\anoconda\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\apps\\anoconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\apps\\anoconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\apps\\anoconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\apps\\anoconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\apps\\anoconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\apps\\anoconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\apps\\anoconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in d:\\apps\\anoconda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\apps\\anoconda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in d:\\apps\\anoconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\apps\\anoconda\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\apps\\anoconda\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\apps\\anoconda\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\apps\\anoconda\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (D:\\apps\\anoconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (D:\\apps\\anoconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (D:\\apps\\anoconda\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 10.1 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 10.6 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.0/12.8 MB 11.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 6.9 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 6.0 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.0/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (D:\\apps\\anoconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (D:\\apps\\anoconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (D:\\apps\\anoconda\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23cd52e1-0520-4dda-974b-aeae9b785d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Latitude\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NER using NLTK ===\n",
      "Sentence 1:\n",
      "  Entity: Barack               | Type: PERSON\n",
      "  Entity: Obama                | Type: PERSON\n",
      "  Entity: Pakistan             | Type: GPE\n",
      "-----------------------------------\n",
      "Sentence 2:\n",
      "  Entity: Apple                | Type: GPE\n",
      "-----------------------------------\n",
      "Sentence 3:\n",
      "-----------------------------------\n",
      "Sentence 4:\n",
      "  Entity: Tesla                | Type: PERSON\n",
      "  Entity: California           | Type: GPE\n",
      "-----------------------------------\n",
      "Sentence 5:\n",
      "  Entity: Lionel               | Type: PERSON\n",
      "  Entity: Messi                | Type: ORGANIZATION\n",
      "  Entity: Inter Miami          | Type: ORGANIZATION\n",
      "-----------------------------------\n",
      "Sentence 6:\n",
      "  Entity: United Nations       | Type: ORGANIZATION\n",
      "  Entity: New York             | Type: GPE\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Download required models if not already present\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sentences = [\n",
    "    \"Barack Obama visited Pakistan in 2010.\",\n",
    "    \"Apple is looking at buying a UK startup for $1 billion.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Tesla released its new model in California.\",\n",
    "    \"Lionel Messi plays for Inter Miami.\",\n",
    "    \"The United Nations held a meeting in New York.\",\n",
    "]\n",
    "\n",
    "print(\"=== NER using NLTK ===\")\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "for idx, sent in enumerate(sentences, 1):\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    ner_tree = nltk.ne_chunk(pos_tags)\n",
    "    print(f\"Sentence {idx}:\")\n",
    "    for subtree in ner_tree:\n",
    "        if hasattr(subtree, 'label'):\n",
    "            entity = \" \".join(token for token, pos in subtree.leaves())\n",
    "            label = subtree.label()\n",
    "            print(f\"  Entity: {entity:20} | Type: {label}\")\n",
    "    print(\"-\" * 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "984009d1-fc42-4168-961f-f36f7bffc56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NER using spaCy ===\n",
      "Sentence 1:\n",
      "  Entity: Barack Obama         | Type: PERSON\n",
      "  Entity: Pakistan             | Type: GPE\n",
      "  Entity: 2010                 | Type: DATE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " visited \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pakistan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2010\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Sentence 2:\n",
      "  Entity: Apple                | Type: ORG\n",
      "  Entity: UK                   | Type: GPE\n",
      "  Entity: $1 billion           | Type: MONEY\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying a \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    UK\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Sentence 3:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The quick brown fox jumps over the lazy dog.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Sentence 4:\n",
      "  Entity: Tesla                | Type: ORG\n",
      "  Entity: California           | Type: GPE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " released its new model in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Sentence 5:\n",
      "  Entity: Messi                | Type: PERSON\n",
      "  Entity: Inter Miami          | Type: LOC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Lionel \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Messi\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " plays for \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Inter Miami\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Sentence 6:\n",
      "  Entity: The United Nations   | Type: ORG\n",
      "  Entity: New York             | Type: GPE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The United Nations\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " held a meeting in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Step 2: NER with spaCy & Visualization\n",
    "\n",
    "# !pip install spacy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(\"=== NER using spaCy ===\")\n",
    "for idx, sent in enumerate(sentences, 1):\n",
    "    doc = nlp(sent)\n",
    "    print(f\"Sentence {idx}:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"  Entity: {ent.text:20} | Type: {ent.label_}\")\n",
    "    # Visualize entities (works in Jupyter/Colab, skip in plain Python)\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "    print(\"-\" * 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b52a2-cca1-4be4-a8f7-8b3f4cd46f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
